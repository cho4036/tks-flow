apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: create-internal-communication
  namespace: argo
spec:
  entrypoint: deploy
  arguments:
    parameters:
    - name: cluster_id
      value: "c81dd656-b75f-4990-adfb-76b0fc9ef7db"

  volumes:
    - name: config
      secret:
        secretName: tks-admin-kubeconfig-secret
    - name: awsconfig
      secret:
        secretName: awsconfig-secret
    - name: artifacts
      configMap:
        name: aws-artifacts
        defaultMode: 0555

  templates:
  - name: deploy
    dag:
      tasks:
      - name: create-securitygroup-for-internal-communication
        template: CreateInternalCon
        dependencies: []
      - name: modify-resource-for-monitoring
        template: FixResources4Prometheus
        dependencies: []
        
  - name: CreateInternalCon
    activeDeadlineSeconds: 1800
    container:
      image: ghcr.io/openinfradev/aws_kubectl:v1.0.0
      command:
      - /bin/bash
      - -exc
      - |
        mkdir ~/.aws
        cp /aws/* ~/.aws/
        mkdir ~/.kube
        cp /kube/value ~/.kube/config

        # Gether VPC info.   
        VPC=$(kubectl get awscluster -n $cluster_id $cluster_id -o=jsonpath={.spec.network.vpc.id})
        CIDR=$(kubectl get awscluster -n $cluster_id $cluster_id -o=jsonpath={.spec.network.vpc.cidrBlock})

        # Create Security Group
        SG=$(aws ec2 create-security-group --group-name taco-internal --description "Security group for interanl communication among nodes" --vpc-id $VPC)
        # Set ingress rule
        aws ec2 authorize-security-group-ingress --group-id $SG --protocol tcp --port 237 --cidr $CIDR

        # Add Security Group to all node in the VPC
        for instance in `aws ec2 describe-instances --filters "Name=vpc-id,Values=$VPC" --query "Reservations[].Instances[].InstanceId"`
        do
          current=$(aws ec2 describe-instance-attribute --attribute "groupSet" --instance-id $instance --query "Groups[].GroupId" --output text)
          aws ec2 modify-instance-attribute --instance-id $instance --groups $current $SG
        done

      env:
      - name: cluster_id
        value: "{{workflow.parameters.cluster_id}}"
      volumeMounts:
      - name: config
        mountPath: "/kube"
      - name: awsconfig
        mountPath: "/aws"

  - name: FixResources4Prometheus
    activeDeadlineSeconds: 1800
    container:
      image: ghcr.io/openinfradev/aws_kubectl:v1.0.0
      command:
      - /bin/bash
      - -exc
      - |
        mkdir ~/.kube
        cp /kube/value ~/.kube/config

        kubectl get secret -n $cluster_id ${cluster_id}-kubeconfig -o=jsonpath='{.data.value}' | base64 -d > ~/.kube/config

        sleep 1000
        # FIX network of kubeproxy to use cluster network
        kubectl patch daemonset -n kube-system kube-proxy -p '{"spec":{"template":{"spec":{"hostNetwork":false}}}}' 

        # FIX binding address of kubeproxy's metric server to 0.0.0.0
        kubectl get cm -n kube-system kube-proxy -o yaml > /tmp/kube-proxy.cm.yaml
        sed -i 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/g' /tmp/kube-proxy.cm.yaml
        kubectl apply -f /tmp/kube-proxy.cm.yaml

      env:
      - name: cluster_id
        value: "{{workflow.parameters.cluster_id}}"
      volumeMounts:
      - name: config
        mountPath: "/kube"
      - name: awsconfig
        mountPath: "/aws"