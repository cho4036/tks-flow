apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: tks-primary-cluster
  namespace: argo
spec:
  entrypoint: set-primary-cluster
  arguments:
    parameters:
    ##########################
    # needed by sub-systems (sub-template)
    ##########################
    - name: site_name  # primary cluster
      value: site3
    - name: revision
      value: "main"
    - name: app_prefix
      value: "{{workflow.parameters.site_name}}"
    - name: repository_base
      value: https://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@github.com/overmon/
    - name: github_account
      value: "decapod10"
    - name: object_store
      value: "s3"

    ##########################
    # For tks-info task #
    ##########################
    - name: tks_info_host
      value: "http://tks-api-dev.taco-cat.xyz:9110"
    - name: organization_id
      value: "o086tb3zc"
    # 아마도 site_name과 동일하게 사용하면 될듯하다.
    - name: cluster_id
      value: "{{workflow.parameters.site_name}}"
    - name: base_repo_branch
      value: develop
    - name: cloud_account_id
      value: ""

  volumes:
  - name: tks-proto-vol
    configMap:
      name: tks-proto
  - name: kubeconfig-adm
    secret:
      secretName: tks-admin-kubeconfig-secret

  templates:
  - name: set-primary-cluster
    synchronization:
      mutex:
        name: primary-cluster-{{workflow.parameters.organization_id}}
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: update-status-mainternance
        template: sub-update-tks-status
        arguments:
          parameters:
          - name: tks_status
            value: MAINTERNANCE
      # - name: get-clusters-in-contract
      #   template: sub-get-cluster

    - - name: set-primary-cluster-on-tks-info
        template: sub-set-primay-cluster-on-tks-info
        arguments:
          parameters:
          - name: cluster_id
            value: "{{workflow.parameters.cluster_id}}"

    # TODO: 전체 완성을 위해서는 아래내역을 구현하여 동적인 bucket을 만드는 방식으로 구현해야 하지만
    #     5월 오픈전 가능한 형상을 위해 협의한 바(아래)에 따라 본부분은 기존 준비됀 것을 사용하는 것으로 구현하고 추후 수정하다.
    #       1. 사용자가 생성하는 첫번째 클러스터는 primary cluster
    #       2. primary cluster는 계약이 종료되기 전까지 임의 삭제불가
    #       3. 개별 클러스터에서 수행되는 모니터링은 없고 계약단위에서 수행되어야 함
    # - - name: prepare-bucket
    #     templateRef:
    #       name: create-application
    #       template: installApps
    #     arguments:
    #       parameters:
    #       - name: list
    #         value: |
    #           [
    #             { "app_group": "tks-cluster", "path": "lma-bucket", "namespace": "taco-system", "target_cluster": "" }
    #           ]

    # 개별 클러스터에 설치됀 loki와 grafana는 지우자
    # - - name: remove-individual-loki-and-grafana
    #     template: sub-remove-individual-loki-and-grafana
    #     arguments:
    #       parameters:
    #       - name: target_clusters
    #         value: '{{steps.get-clusters-in-contract.outputs.parameters.primary_cluster}} {{steps.get-clusters-in-contract.outputs.parameters.member_clusters}}'

    - - name: federation-components-preinstall-for-minio
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "minio", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "loki", "namespace": "lma", "target_cluster": "" }
              ]
        when: "{{workflow.parameters.object_store}} == minio"

    - - name: pre-change-target
        template: sub-pre-change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'
        when: "{{workflow.parameters.object_store}} == s3"

    - - name: render-pre-modified-clusters
        templateRef:
          name: event-gitea-render-manifests
          template: main
        arguments:
          parameters:
          - name: decapod_site_repo
            value: "{{ workflow.parameters.github_account }}/{{item}}"
          - name: base_repo_branch
            value: "{{ workflow.parameters.base_repo_branch }}"
        withParam: "{{ steps.pre-change-target.outputs.parameters.modified_cluster_list}}"

    - - name: federation-components-preinstall-for-s3
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "lma-bucket", "namespace": "taco-system", "target_cluster": "" },
                { "app_group": "lma", "path": "loki", "namespace": "lma", "target_cluster": "" }
              ]
        when: "{{workflow.parameters.object_store}} == s3"

    # TODO: 전체 완성을 위해서는 아래내역을 구현하여 동적인 bucket을 만드는 방식으로 구현해야 하지만
    #     5월 오픈전 가능한 형상을 위해 협의한 바(아래)에 따라 본부분은 기존 준비됀 것을 사용하는 것으로 구현하고 추후 수정하다.
    #       1. 사용자가 생성하는 첫번째 클러스터는 primary cluster
    #       2. primary cluster는 계약이 종료되기 전까지 임의 삭제불가
    #       3. 개별 클러스터에서 수행되는 모니터링은 없고 계약단위에서 수행되어야 함
    # 하지만 이부분에 datasource 바꿔주는 부분을 포함하고 있으므로 일단 한번 타야할듯...
    - - name: change-target
        template: change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: federation-components-postinstall
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "thanos-config", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "thanos", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "grafana", "namespace": "lma", "target_cluster": "" }
              ]

    - - name: update-status-done
        template: sub-update-tks-status
        arguments:
          parameters:
          - name: tks_status
            value: Running

  - name: change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: change-target
        template: sub-change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: render-modified-clusters
        templateRef:
          name: event-gitea-render-manifests
          template: main
        arguments:
          parameters:
          - name: decapod_site_repo
            value: "{{ workflow.parameters.github_account }}/{{item}}"
          - name: base_repo_branch
            value: "{{ workflow.parameters.base_repo_branch }}"
        withParam: "{{ steps.change-target.outputs.parameters.modified_cluster_list}}"

  - name: update-eps-for-thanos
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: sync-organization-changes
        template: sub-sync-organization-changes
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: render-primary-cluster
        templateRef:
          name: event-gitea-render-manifests
          template: main
        arguments:
          parameters:
          - name: decapod_site_repo
            value: "{{ workflow.parameters.github_account }}/{{steps.sync-organization-changes.outputs.parameters.primary_cluster}}"
          - name: base_repo_branch
            value: "{{ workflow.parameters.base_repo_branch }}"
        when: "{{steps.sync-organization-changes.outputs.parameters.primary_cluster}} != 'NO_CHANGE_HERE'"

  #######################
  # Template Definition #
  #######################
  - name: sub-update-tks-status
    inputs:
      parameters:
      - name: tks_status
    container:
      name: update-tks-status
      image: harbor-cicd.taco-cat.xyz/tks/hyperkube:v1.18.6
      command:
      - /bin/bash
      - '-c'
      - |
        echo "check tks status"
        echo "if status is not running then throw exception~~"
        echo "connect tks-info server and change the {{workflow.parameters.organization_id}} status to mode:{{inputs.parameters.tks_status}}"
    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2

  # - name: sub-prepare-bucket
  #   inputs:
  #     parameters:
  #     - name: primary_cluster
  #   container:
  #     name: prepare-bucket
  #     image: harbor-cicd.taco-cat.xyz/tks/hyperkube:v1.18.6
  #     command:
  #     - /bin/bash
  #     - '-c'
  #     - |
  #       echo "prepare bucket for the '{{workflow.parameters.organization_id}}' (clusters: '{{inputs.parameters.primary_cluster}}')"
  #   activeDeadlineSeconds: 900
  #   retryStrategy:
  #     limit: 2

  - name: sub-pre-change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-target-changer
      image: harbor-cicd.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        empty_char=

        if [ -z ${primary_cluster} ] || [ "${primary_cluster}" = "$empty_char" ]; then
          primary_cluster=${current_cluster}
        fi

        S3_Service="s3://ap-northeast-2"
        cp /kube/value kubeconfig_adm
        export KUBECONFIG=kubeconfig_adm

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_base=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        else
          repository_base=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        fi

        # Use AWS STS temporary security credential if multi-tenancy
        if [ -z $CLOUD_ACCOUNT_ID ]; then
          iamPrefix="arn:aws:iam::$(kubectl get secret -n argo aws-account-id -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d):role"
        else
          ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
          iamPrefix=${ROLE_ARN%/*}
        fi

        for member in $member_clusters
        do
          # 1. endpoint of fb on eachcluster
          log "INFO" "##### change the loki target to $LOKI_HOST:$LOKI_PORT and $S3_Service (the current target is ${member})"
          [ -d ${member} ] || git clone ${repository_base}${member}
          cd ${member}

          yq -i e "del(.charts[] | select(.name == \"thanos-config\").override.objectStorage)" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.type=\"s3\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.endpoint=\"s3.ap-northeast-2.amazonaws.com\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.region=\"ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.bucket=\"${primary_cluster}-tks-thanos\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.signature_version2=false)" ${member}/lma/site-values.yaml
          yq -i e  ".global.clusterName=\"${member}\"" ${member}/lma/site-values.yaml

          yq -i e "del(.charts[] | select(.name == \"loki\").override.loki.storageConfig.aws)" ${member}/lma/site-values.yaml
          yq  -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.s3=\"s3://ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq  -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.dynamodb.dynamodb_url=\"dynamodb://ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq  -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.bucketnames=\"${primary_cluster}-tks-loki\")" ${member}/lma/site-values.yaml

          if [ `kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
            if [ -z $iamRoles ]; then
              iamRoles="\"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
            else
              iamRoles="${iamRoles}, \"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
            fi
          else
            log "WARN" "Cluster(${member}) has no node role for taco-lma"
          fi

          cd -
        done
        yq -i e  ".global.iamRoles=[${iamRoles}]" ${primary_cluster}/${primary_cluster}/lma/site-values.yaml

        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        for member in $member_clusters
        do
          cd ${member}
          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes on ${member} to use s3"
            cmessage="changes on ${member} to use s3"
            git add ${member}/lma/site-values.yaml
            git commit -m "change loki and thanos endpoints. (by set-primary workflow)" -m "$cmessage"
            git push
            modified_clusters="${modified_clusters} ${member}"
            # echo -n "${member} " >>  /mnt/out/modified_cluster_list.txt
          else
            log "INFO" "No change on the cluster ${member}"
          fi
          cd -
          rm -rf ${member}
        done
        jq -n '$ARGS.positional' --args $modified_clusters >  /mnt/out/modified_cluster_list.txt

      env:
      - name: OBJECT_SOTRE
        value: "{{workflow.parameters.object_store}}"
      envFrom:
      - secretRef:
          name: "git-svc-token"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: modified_cluster_list
        valueFrom:
          path: /mnt/out/modified_cluster_list.txt
    activeDeadlineSeconds: 900

  - name: sub-change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-target-changer
      image: harbor-cicd.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        empty_char=

        if [ -z ${primary_cluster} ] || [ "${primary_cluster}" = "$empty_char" ]; then
          primary_cluster=${current_cluster}
        fi

        #################
        # Check endpoints from the primary_cluster
        #################
        primary_kube_secret=$(kubectl get secret -n ${primary_cluster} ${primary_cluster}-tks-kubeconfig -o jsonpath="{.data.value}" | base64 -d)
        # echo -e "primary_kube_secret:\n$primary_kube_secret" | head -n 5
        cat <<< "$primary_kube_secret" > kubeconfig

        while [ -z $(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.status.loadBalancer.ingress[*].hostname}") ]
        do
          if [ "$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.spec.type}")" -neq "LoadBalancer" ]; then
            log "FAIL" "The infras on primary are not cofigured properly.(No LoadBalancer)"
            exit -1
          fi

          echo "Waiting for generating the loadbalancer of LOKI(3s)"
          sleep 3
        done

        LOKI_HOST=$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
        LOKI_PORT=$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.spec.ports[0].port}")

        if [ "$OBJECT_SOTRE" == "minio" ]; then
          S3_HOST=$(kubectl --kubeconfig=kubeconfig get svc -n lma minio -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
          S3_PORT=$(kubectl --kubeconfig=kubeconfig get svc -n lma minio -o jsonpath="{.spec.ports[0].port}")
          S3_Service=${S3_HOST}:${S3_PORT}
        fi

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_base=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        else
          repository_base=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        fi

        for member in $member_clusters
        do
          # 1. endpoint of fb on eachcluster
          log "INFO" "##### change the loki target to $LOKI_HOST:$LOKI_PORT and $S3_Service (the current target is ${member})"
          [ -d ${member} ] || git clone ${repository_base}${member}
          cd ${member}

          yq -i e  ".global.lokiHost=\"${LOKI_HOST}\"" ${member}/lma/site-values.yaml
          yq -i e  ".global.lokiPort=\"${LOKI_PORT}\"" ${member}/lma/site-values.yaml
          if [ "$OBJECT_SOTRE" == "minio" ]; then
            yq -i e  ".global.s3Service=\"${S3_Service}\"" ${member}/lma/site-values.yaml
          fi

          yq -i e  ".global.clusterName=\"${member}\"" ${member}/lma/site-values.yaml

          # 2. grafana datasource on primay_cluster
          if [ ${member} = ${primary_cluster} ]; then
            yq -i e  ".global.grafanaDatasourceMetric=\"thanos-query.lma:9090\"" ${member}/lma/site-values.yaml
            yq -i e  ".global.TksWebhookUrl=\"{{workflow.parameters.alert_tks}}\"" ${member}/lma/site-values.yaml
            yq -i e  ".global.SlackUrl=\"{{workflow.parameters.alert_slack}}\"" ${member}/lma/site-values.yaml
          else
            yq -i e  ".global.grafanaDatasourceMetric=\"lma-prometheus.lma:9090\"" ${member}/lma/site-values.yaml
          fi

          cd -
        done

        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        for member in $member_clusters
        do
          cd ${member}
          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes on ${member} to $LOKI_HOST:$LOKI_PORT and $S3_Service"
            if [ "$OBJECT_SOTRE" == "minio" ]; then
              cmessage="the loki to $LOKI_HOST:$LOKI_PORT and grafana to $S3_Service (cluster ${member})"
            else
              cmessage="the loki to $LOKI_HOST:$LOKI_PORT (cluster ${member})"
            fi
            git add ${member}/lma/site-values.yaml
            git commit -m "change loki and thanos endpoints. (by set-primary workflow)" -m "$cmessage"
            git push
            modified_clusters="${modified_clusters} ${member}"
            # echo -n "${member} " >>  /mnt/out/modified_cluster_list.txt
          else
            log "INFO" "No change on the cluster ${member}"
          fi
          cd -
          rm -rf ${member}
        done
        jq -n '$ARGS.positional' --args $modified_clusters >  /mnt/out/modified_cluster_list.txt

      env:
      - name: OBJECT_SOTRE
        value: "{{workflow.parameters.object_store}}"
      envFrom:
      - secretRef:
          name: "git-svc-token"
      volumeMounts:
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: modified_cluster_list
        valueFrom:
          path: /mnt/out/modified_cluster_list.txt
    activeDeadlineSeconds: 900

  - name: sub-sync-organization-changes
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-thanos-store-changer
      image: harbor-cicd.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        # empty_char=

        if [ -z ${primary_cluster} ] || [ "${primary_cluster}" = "$empty_char" ]; then
          primary_cluster=${current_cluster}
        fi

        #################
        # Gather endpoints
        #################
        # Use AWS STS temporary security credential if multi-tenancy
        eplist=\"thanos-storegateway:10901\"
        for member in $member_clusters
        do
          # Thanos Endpoints
          kube_secret=$(kubectl get secret -n ${member} ${member}-kubeconfig -o jsonpath="{.data.value}" | base64 -d)
          cat <<< "$kube_secret" > kubeconfig
          if [ `kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
            while [ -z $(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.status.loadBalancer.ingress[*].hostname}") ]
            do
              if [ "$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.spec.type}")" -neq "LoadBalancer" ]; then
                log "FAIL" "A service for the thanos-sidcar in ${member} is not cofigured properly.(No LoadBalancer)"
                exit -1
              fi

              echo "Waiting for generating the loadbalancer of Prometheus thanos-sidecar(3s)"
              sleep 3
            done

            eplist="${eplist}, \"$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.status.loadBalancer.ingress[0].hostname}"):$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.spec.ports[0].port}")\""
          else
            log "WARN" "Cluster(${member}) has no prometheus sidecar"
          fi
        done

        cp /kube/value kubeconfig_adm
        export KUBECONFIG=kubeconfig_adm

        if [ "$OBJECT_SOTRE" == "s3" ]; then
          #################
          # Gather iams
          #################
          # Use AWS STS temporary security credential if multi-tenancy
          if [ -z $CLOUD_ACCOUNT_ID ]; then
            iamPrefix="arn:aws:iam::$(kubectl  get secret -n argo aws-account-id -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d):role"
          else
            ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
            iamPrefix=${ROLE_ARN%/*}
          fi
          
          for member in $member_clusters
          do
            if [ `kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
              if [ -z $iamRoles ]; then
                iamRoles="\"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
              else
                iamRoles="${iamRoles}, \"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
              fi
            else
              log "WARN" "Cluster(${member}) has no node role for taco-lma"
            fi
          done
        fi

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_url=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/${primary_cluster}
        else
          repository_url=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/${primary_cluster}
        fi

        git clone ${repository_url}
        cd ${primary_cluster}
        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        yq -i e  ".global.thanosQueryStores=[${eplist}]" ${primary_cluster}/lma/site-values.yaml

        if [[ `git status --porcelain` ]]; then
          log "INFO" "##### commit changes endpoints for the thanos query on ${primay_cluster} "
          cmessage="changes endpoints for the thanos query on ${primay_cluster}"
          git add ${primary_cluster}/lma/site-values.yaml
          git commit -m "change thanos-query stores. (by set-primary workflow)" -m "$cmessage"
          git push
          echo ${primary_cluster} > /mnt/out/primary_cluster.txt
        else
          log "INFO" "No change on the cluster ${member}"
          echo NO_CHANGE_HERE > /mnt/out/primary_cluster.txt
        fi

        if [ "$OBJECT_SOTRE" != "s3" ]; then
          yq -i e  ".global.iamRoles=[${iamRoles}]" ${primary_cluster}/lma/site-values.yaml

          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes iamRoles for the s3 on ${primay_cluster} "
            cmessage="changes iamRoles for the s3 on ${primay_cluster}"
            git add ${primary_cluster}/lma/site-values.yaml
            git commit -m "change iamRoles(s3). (by set-primary workflow)" -m "$cmessage"
            git push
            echo ${primary_cluster} > /mnt/out/primary_cluster.txt
          else
            log "INFO" "(iamRoles) No change on the cluster ${member}"
          fi
        fi

        cd -
        rm -rf ${primary_cluster}

      envFrom:
      - secretRef:
          name: "git-svc-token"
      env:
      - name: CLUSTER_ID
        value: "{{workflow.parameters.cluster_id}}"
      - name: CLOUD_ACCOUNT_ID
        value: "{{workflow.parameters.cloud_account_id}}"
      - name: OBJECT_SOTRE
        value: "{{workflow.parameters.object_store}}"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: primary_cluster
        valueFrom:
          path: /mnt/out/primary_cluster.txt
    activeDeadlineSeconds: 900


  - name: sub-remove-individual-loki-and-grafana
    inputs:
      parameters:
      - name: target-clusters
    container:
      name: delete-loki
      image: harbor-cicd.taco-cat.xyz/tks/hyperkube:v1.18.6
      command:
      - /bin/bash
      - '-c'
      - |
        # log into Argo CD server
        ./argocd login $ARGO_SERVER --plaintext --insecure --username $ARGO_USERNAME \
        --password $ARGO_PASSWORD

        for cid in {{inputs.parameters.target-clusters}}
        do
          for app in loki grafana
          do
            APP=${cid}-${app}

            # Pre-check: validate if the app exists
            if ! (./argocd app list --output name | grep -E "^$APP$"); then
              echo "No such app: $APP. Skipping app removal.."
              exit 1
            fi
          done
        done

        for cid in {{inputs.parameters.target-clusters}}
        do
          for app in loki grafana
          do
            APP=${cid}-${app}
            echo "Found app '$APP'. Start deleting it.."
            ./argocd app delete $APP --cascade -y

            while (./argocd app list --output name | grep -E "^$APP$" )
            do
              echo "Waiting 20 secs for the app to be deleted.."
              sleep 20
            done

            echo "App '$APP' have been deleted!"
          done
        done

      envFrom:
        - secretRef:
            name: "decapod-argocd-config"
    activeDeadlineSeconds: 900

  - name: sub-get-cluster
    synchronization:
      mutex:
        name: primary-cluster-{{workflow.parameters.organization_id}}
    volumes:
    - name: out
      emptyDir: {}
    script:
      image: harbor-cicd.taco-cat.xyz/tks/centos-tks-api:v1.0
      command: ["python"]
      env:
      - name: PYTHONPATH
        value: "/opt/protobuf/:/opt/rh/rh-python38/root/lib/python3.8/site-packages/:/opt/app-root/lib/python3.8/site-packages/"
      - name: TKS_API_URL
        value: "{{workflow.parameters.tks_info_host}}"
      envFrom:
      - secretRef:
          name: "tks-api-secret"
      volumeMounts:
      - name: out
        mountPath: /mnt/out
      source: |
        import sys
        import requests
        import json
        import os

        TKS_API_URL = "{{workflow.parameters.tks_info_host}}"
        ORGANIZATION_ID = "{{workflow.parameters.organization_id}}"
        CLUSTER_ID = "{{workflow.parameters.cluster_id}}"

        def getToken() :
          data = {
              'organizationId' : os.environ['ORGANIZATION_ID'],
              'accountId': os.environ['ACCOUNT_ID'],
              'password' : os.environ['PASSWORD']
          }

          res = requests.post(TKS_API_URL+"/api/1.0/auth/login", json = data )
          if res.status_code != 200 :
              return ''
          resJson = res.json()
          return resJson['user']['token']

        # get primary organization id
        res = requests.get(TKS_API_URL+"/api/1.0/organizations/" + ORGANIZATION_ID, headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"} )
        if res.status_code != 200 :
          sys.exit('Failed to get organization')

        print(res.text)
        organization = res.json()['organization']

        with open("/mnt/out/primary_cluster.txt", "w") as f:
          # primary_cluster = json.dumps(organization.get('phone')) # for test without db manifulation
          primary_cluster = json.dumps(organization.get('primaryClusterId'))
          print('Primary cluster: ', primary_cluster)
          f.write(primary_cluster)

        # get cluster ids
        res = requests.get(TKS_API_URL+"/api/1.0/clusters?organizationId=" + ORGANIZATION_ID, headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"} )
        if res.status_code != 200 :
          sys.exit('Failed to get clusters')

        # print(res.text)
        clusters = res.json()['clusters']

        with open("/mnt/out/member_clusters.txt", "w") as f:
          for cluster in clusters:
            if cluster['status']=="RUNNING":
              print('Member cluster: ',cluster['id'])
              f.write(cluster['id']+' ')

    outputs:
      parameters:
      - name: primary_cluster
        valueFrom:
          path: /mnt/out/primary_cluster.txt
      - name: member_clusters
        valueFrom:
          path: /mnt/out/member_clusters.txt

    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2

  - name: sub-set-primay-cluster-on-tks-info
    inputs:
      parameters:
      - name: cluster_id
    script:
      image: harbor-cicd.taco-cat.xyz/tks/centos-tks-api:v1.0
      command: ["python"]
      env:
      - name: PYTHONPATH
        value: "/opt/protobuf/:/opt/rh/rh-python38/root/lib/python3.8/site-packages/:/opt/app-root/lib/python3.8/site-packages/"
      - name: TKS_API_URL
        value: "{{workflow.parameters.tks_info_host}}"
      envFrom:
      - secretRef:
          name: "tks-api-secret"
      source: |
        import sys
        import requests
        import json
        import os

        TKS_API_URL = "{{workflow.parameters.tks_info_host}}"
        ORGANIZATION_ID = "{{workflow.parameters.organization_id}}"
        CLUSTER_ID = "{{inputs.parameters.cluster_id}}"

        def getToken() :
          data = {
              'organizationId' : os.environ['ORGANIZATION_ID'],
              'accountId': os.environ['ACCOUNT_ID'],
              'password' : os.environ['PASSWORD']
          }

          res = requests.post(TKS_API_URL+"/api/1.0/auth/login", json = data )
          if res.status_code != 200 :
              return ''
          resJson = res.json()
          return resJson['user']['token']

        # set primary cluster with this cluster
        # api/1.0/organizations/c3c16juq5/primary-cluster
        res = requests.patch(TKS_API_URL+"/api/1.0/organizations/"+ ORGANIZATION_ID + "/primary-cluster",
                              headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"},
                              data=json.dumps({"primaryClusterId": CLUSTER_ID})
                            )
        if res.status_code != 200 :
          print(res.reason)
          sys.exit('Failed to set primary-cluster at organization')

    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2